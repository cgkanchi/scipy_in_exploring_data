{
 "metadata": {
  "name": "",
  "signature": "sha256:3417305ed2f6c38ea69959f70fd7432753b88f088ea34750d896144fef2479a6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Processing Real-World Data with the SciPy stack."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Table of Contents"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- [Introduction](#Introduction)\n",
      "    - [Requirements](#Requirements)\n",
      "        - [Windows and Mac OS X](#Windows-and-Mac-OS-X)\n",
      "        - [Linux or other *NIX](#Linux/Other-*NIX)\n",
      "    - [Downloading the data](#Downloading-the-data)\n",
      "    \n",
      "- [Loading data into Python](#Loading-data-into-Python)\n",
      "    - [Using the read_* methods from Pandas](#Using-the-read_*-methods-from-Pandas)\n",
      "    - [Didn't work, why?](#Didn't-work,-why?)\n",
      "    - [Still doesn't work?!](#Still-doesn't-work?!)\n",
      "    - [Screw it, let's just write our own parser](#Screw-it,-let's-just-write-our-own-parser)\n",
      "    - [Close, but not quite!](#Close,-but-not-quite!)\n",
      "    \n",
      "- [Fun with missing data](#Fun-with-missing-data)\n",
      "    - [That's a lot of NaNs!](#That's-a-lot-of-NaNs!)\n",
      "    - [Interpolating data](#Interpolating-data)\n",
      "        - [Checking interpolated data](#Checking-interpolated-data)\n",
      "        - [Warnings about iterpolation](#Warnings-about-interpolation)\n",
      "    - [Dealing with invalid data](#Dealing-with-invalid-data)\n",
      "    \n",
      "- [Indexing and selecting data](#Indexing-and-selecting-data)\n",
      "    - [Basic indexing](#Basic-indexing)\n",
      "    - [Grouping data](#Grouping-data)\n",
      "    \n",
      "- [Analysing and visualising data](#Analysing-and-visualising-data)\n",
      "    - [Plotting timeseries data with Seaborn](#Plotting-timeseries-data-with-Seaborn)\n",
      "    - [Deriving new channels from data](#Deriving-new-channels-from-data)\n",
      "    \n",
      "- [General tips](#General-tips)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When people get started with scientific programming and data science, API examples and library documentation usually use very simple or pseudorandom data. While this is very useful to demonstrate individual APIs, it doesn't reflect the true workflow of a data scientist. Real-world data is almost invariably imperfect, with missing values, horrible data formats and other issues which make analysis difficult.\n",
      "\n",
      "What follows is an attempt to use real-world engineering data from the world of motorsport to illustrate how a data scientist may go about dealing with data handed to him/her, including some common pitfalls."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Requirements"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Python 2.7.x\n",
      "- IPython\n",
      "- Numpy\n",
      "- Scipy\n",
      "- Matplotlib\n",
      "- Pandas \n",
      "- Seaborn\n",
      "- Matplotlib Basemap\n",
      "- Requests [optional, only required to redownload data]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Windows and Mac OS X"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For Windows and OS X users, the easiest way to get all these dependencies is to install a pre-packaged Python distribution that includes most of these packages. Continuum's Anaconda is a good choice for non-production work.\n",
      "\n",
      "#####Continuum Anaconda - https://store.continuum.io/cshop/anaconda/\n",
      "\n",
      "- Install Anaconda using the graphical installer\n",
      "- At the command prompt/terminal: `conda install basemap seaborn` "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Linux/Other *NIX"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use your package manager or pip to get the requisite packages.\n",
      "\n",
      "On Ubuntu or other Debian-based systems, you can\n",
      "\n",
      "    $ sudo apt-get install python-numpy python-scipy python-matplotlib python-pip python-mpltoolkits.basemap python \\\n",
      "    && sudo pip install seaborn"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Downloading the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is not necessary - a downloaded version is provided in the `data/json` directory.\n",
      "\n",
      "Data was downloaded from `http://race-capture.com:8081/event/5328c5529da60f8962000001/device/52a36c6c0a5a2b979c00000b/lap/<lap_number>`. \n",
      "\n",
      "The data was chosen for two reasons:\n",
      "\n",
      "- Data is in a non-standard JSON-like format\n",
      "- Data is \"imperfect\" in that there are issues with multiple sensors recording at different frequencies and missing data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Loading data into Python"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using the read_* methods from Pandas"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pandas has a lot of methods in `pandas.io`\n",
      "\n",
      "- Can use CSV, JSON, HTML, Excel, HDF, even SQL\n",
      "- You can even make a DataFrame from Django QuerySets!\n",
      "\n",
      "We have JSON data, so we'd use `pandas.read_json()`\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import sys, os\n",
      "import pandas as pd\n",
      "import zipfile\n",
      "\n",
      "def load_json():\n",
      "    with zipfile.ZipFile('data/json/json.zip', 'r', zipfile.ZIP_DEFLATED) as data_zip:\n",
      "        file_list = (fname for fname in data_zip.namelist() if os.path.splitext(fname)[1].lower()=='.json')\n",
      "        data = []\n",
      "        for json_file in file_list:\n",
      "            df = pd.read_json(data_zip.read(json_file))\n",
      "            data_df.append(df)\n",
      "        \n",
      "        data_df = pd.DataFrame(data)\n",
      "\n",
      "load_json()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Didn't work, why? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take a look at one of these files:\n",
      "\n",
      "    {\"eventDeviceId\":\"532db3f29da60f7fb800001d\",\"timestamp\":{\"$date\":\"2014-03-22T17:28:28.144Z\"},\"values\":[{\"name\":\"Position\",\"latitude\":38.161358,\"longitude\":-122.454376},{\"name\":\"Latency\",\"value\":5941},{\"name\":\"OilPress\",\"value\":37.49},{\"name\":\"FuelPress\",\"value\":48.94},{\"name\":\"Coolant\",\"value\":184.28},{\"name\":\"Battery\",\"value\":13.71},{\"name\":\"AccelX\",\"value\":-0.074},{\"name\":\"AccelY\",\"value\":-0.017},{\"name\":\"AccelZ\",\"value\":1.013},{\"name\":\"Yaw\",\"value\":0},{\"name\":\"Speed\",\"value\":19.83},{\"name\":\"Time\",\"value\":172822.203},{\"name\":\"LapCount\",\"value\":1},{\"name\":\"LapTime\",\"value\":6.105},{\"name\":\"Distance\",\"value\":0.001}],\"tick\":8758}\n",
      "    {\"eventDeviceId\":\"532db3f29da60f7fb800001d\",\"timestamp\":{\"$date\":\"2014-03-22T17:28:28.144Z\"},\"values\":[{\"name\":\"Position\",\"latitude\":38.161362,\"longitude\":-122.454391},{\"name\":\"Latency\",\"value\":5848},{\"name\":\"AccelX\",\"value\":-0.053},{\"name\":\"AccelY\",\"value\":-0.022},{\"name\":\"AccelZ\",\"value\":0.994},{\"name\":\"Yaw\",\"value\":0},{\"name\":\"Speed\",\"value\":19.64},{\"name\":\"Time\",\"value\":172822.297},{\"name\":\"Distance\",\"value\":0.002}],\"tick\":8759}\n",
      "    ...\n",
      "    \n",
      "- Invalid JSON - Just individual objects, not in an array (not enclosed in [])\n",
      "- No commas between objects\n",
      "- So we edit code to add these things"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os\n",
      "import pandas as pd\n",
      "import zipfile\n",
      "import re\n",
      "\n",
      "def load_json():\n",
      "    with zipfile.ZipFile('data/json/json.zip', 'r', zipfile.ZIP_DEFLATED) as data_zip:\n",
      "        file_list = (fname for fname in data_zip.namelist() if os.path.splitext(fname)[1].lower()=='.json')\n",
      "        data = []\n",
      "        for json_file in file_list:\n",
      "            json_data = data_zip.read(json_file)                \n",
      "            json_data = re.sub(r'}[^,\\]]', '},', json_data)\n",
      "            json_data = '[' + json_data + ']'\n",
      "            df = pd.read_json(json_data)\n",
      "            data_df.append(df)\n",
      "       \n",
      "        data_df = pd.DataFrame(data)\n",
      "\n",
      "load_json()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Still doesn't work?!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Unhelpful error message :(\n",
      "- Go through pandas source code to find error?\n",
      "- Or, cheat and use a JSON validator - http://jsonlint.com"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import zipfile, re\n",
      "\n",
      "with zipfile.ZipFile('data/json/json.zip', 'r', zipfile.ZIP_DEFLATED) as data_zip:\n",
      "    json_file = list(fname for fname in data_zip.namelist() if os.path.splitext(fname)[1].lower()=='.json')[0]\n",
      "    json_data = data_zip.read(json_file)                \n",
      "    json_data = re.sub(r'}[^,\\]]', '},', json_data)\n",
      "    json_data = '[' + json_data.strip().rstrip(',') + ']'    \n",
      "    with open('json_validate.txt', 'w') as f:\n",
      "        f.write(json_data)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have a trailing comma!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os\n",
      "import pandas as pd\n",
      "import zipfile\n",
      "import re\n",
      "\n",
      "def load_json():\n",
      "    with zipfile.ZipFile('data/json/json.zip', 'r', zipfile.ZIP_DEFLATED) as data_zip:\n",
      "        file_list = (fname for fname in data_zip.namelist() if os.path.splitext(fname)[1].lower()=='.json')\n",
      "        data = []\n",
      "        for json_file in file_list:\n",
      "            json_data = data_zip.read(json_file)                \n",
      "            json_data = re.sub(r'}[^,\\]]', '},', json_data)\n",
      "            json_data = '[' + json_data.rstrip(',') + ']'\n",
      "            df = pd.read_json(json_data)\n",
      "            data.append(df)\n",
      "       \n",
      "        data_df = pd.DataFrame(data_df)\n",
      "\n",
      "load_json()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Screw it, let's just write our own parser"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os\n",
      "import pandas as pd\n",
      "import zipfile\n",
      "import re\n",
      "import json\n",
      "\n",
      "def load_json():\n",
      "    with zipfile.ZipFile('data/json/json.zip', 'r', zipfile.ZIP_DEFLATED) as data_zip:\n",
      "        file_list = (fname for fname in data_zip.namelist() if os.path.splitext(fname)[1].lower()=='.json')\n",
      "        data = []\n",
      "        for json_file in file_list:\n",
      "            with data_zip.open(json_file) as jf:\n",
      "                json_data = [json.loads(line) for line in jf]\n",
      "                data.extend(json_data)\n",
      "       \n",
      "    data_df = pd.DataFrame(data)\n",
      "    return data_df\n",
      "        \n",
      "\n",
      "df = load_json()\n",
      "print df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Close, but not quite!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Some data has been extracted into columns correctly, but nested arrays have just been extracted as text. \n",
      "- So, we have to parse the nested arrays correctly\n",
      "- These kinds of bugs are why you attempt to use library functions first!!\n",
      "- Take a close look at the data files again"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os\n",
      "import pandas as pd\n",
      "import zipfile\n",
      "import re\n",
      "import json\n",
      "\n",
      "def load_json():\n",
      "    with zipfile.ZipFile('data/json/json.zip', 'r', zipfile.ZIP_DEFLATED) as data_zip:\n",
      "        file_list = (fname for fname in data_zip.namelist() if os.path.splitext(fname)[1].lower()=='.json')\n",
      "        data = []\n",
      "        for json_file in file_list:\n",
      "            with data_zip.open(json_file) as jf:\n",
      "                lap_number = int(os.path.splitext(os.path.split(json_file)[-1])[0][4:])               \n",
      "                for line in jf:\n",
      "                    line = json.loads(line)                    \n",
      "                    if line['values']:                        \n",
      "                        line_values = {d['name']: d['value'] for d in line['values'] if d['name'] not in ('Position', 'LapCount')}\n",
      "                        line_values['lap_number'] = lap_number\n",
      "                        try:\n",
      "                            line_values['longitude'],  line_values['latitude'] = [(d['longitude'], d['latitude']) \n",
      "                                    for d in line['values'] if d['name'] == 'Position'][0]\n",
      "                        except IndexError:\n",
      "                            pass\n",
      "                        \n",
      "                        try:\n",
      "                            line_values['tick'] = line['tick']\n",
      "                        except KeyError:\n",
      "                            line_values['tick'] = float('nan')                        \n",
      "                            \n",
      "                    data.append(line_values)\n",
      "       \n",
      "    data_df = pd.DataFrame(data)\n",
      "    return data_df\n",
      "        \n",
      "\n",
      "df = load_json()\n",
      "df.to_csv('data/csv/data.csv')\n",
      "print 'Done'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Fun with missing data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "That's a lot of NaNs!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv('data/csv/data.csv')\n",
      "print df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- NaN means \"not a number\"\n",
      "- Basically, data is missing\n",
      "- Why?\n",
      "    - In this case, sensors are recording data at different frequencies\n",
      "    - So we don't have data from all sensors at all data points\n",
      "    - Sometimes, data are just missing \n",
      "        - Data collection failure\n",
      "        - Sensor failure\n",
      "        - Communication failure\n",
      "        \n",
      "- Always try to find out *why*. Your next steps depend on this.\n",
      "        \n",
      "- So what do we do?\n",
      "    - Interpolate the data - Great for numerical data\n",
      "    - Not good for qualitative data - **Don't interpolate qualitative data**\n",
      "        - Use `fillna()` instead"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Interpolating data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv('data/csv/data.csv')\n",
      "df = df.interpolate()\n",
      "print df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Checking interpolated data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Plot original vs interpolated\n",
      "- Look for discrepancies"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "\n",
      "df = pd.read_csv('data/csv/data.csv')\n",
      "df_int = df.interpolate()\n",
      "\n",
      "def plot_lap(df1, df2, lap_number):\n",
      "    plt.plot(df1[df1['lap_number']==lap_number]['longitude'], df1[df1['lap_number']==lap_number]['latitude'], label='Raw data')\n",
      "    plt.plot(df2[df2['lap_number']==lap_number]['longitude'], df2[df2['lap_number']==lap_number]['latitude'], label='Interpolated data')\n",
      "    plt.legend(loc='best')\n",
      "    plt.title('Lap %d'%lap_number)\n",
      "    \n",
      "#show figures inside the notebook\n",
      "%matplotlib inline \n",
      "\n",
      "plot_lap(df, df_int, 1)\n",
      "plt.figure()\n",
      "plot_lap(df, df_int, 120)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Warnings about interpolation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Default interpolation uses the \"index\"\n",
      "- Default interpolation method is \"linear\"\n",
      "    - Fastest, but doesn't always work - check the values!\n",
      "    - Cubic or spline interpolation is often better, but (often much) slower \n",
      "    - So how to check?\n",
      "- Be very careful with integer values. Pandas is usually sensible, but check anyway!\n",
      "- Don't use float indexes if you wish to interpolate!\n",
      "    - Might get fixed\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Dealing with invalid data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Many data will have valid ranges\n",
      "    - For example, longitude ranges from -180 to +180 and latitude ranges from -90 to +90\n",
      "- Pandas has very powerful built-in indexing, allowing you to get rid of invalid data\n",
      "    - You can remove rows with invalid data \n",
      "        - **But** you might be throwing away _valid_ data in other columns\n",
      "        - So it's better to replace them with NaNs\n",
      "    - Be careful while setting values using indexing, always use `df.loc` or `df.ix` to ensure that the data are replaced"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "df = pd.read_csv('data/csv/data.csv')\n",
      "\n",
      "invalid_lats = df.index[(df['latitude'].abs() >= 90)]\n",
      "df['latitude'].loc[invalid_lats] = float('nan')\n",
      "invalid_lons = df.index[(df['longitude'].abs() >= 180)]\n",
      "df['longitude'].loc[invalid_lons] = float('nan')\n",
      "\n",
      "%matplotlib inline \n",
      "plt.plot(df['longitude'])\n",
      "plt.figure()\n",
      "plt.plot(df['latitude'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "df = pd.read_csv('data/csv/data.csv')\n",
      "\n",
      "invalid_lats = df.index[(df['latitude'].abs() >= 90)]\n",
      "df['latitude'].loc[invalid_lats] = float('nan')\n",
      "invalid_lons = df.index[(df['longitude'].abs() >= 180)]\n",
      "df['longitude'].loc[invalid_lons] = float('nan')\n",
      "\n",
      "lat_mean, lat_std = df['latitude'].mean(), df['latitude'].std()\n",
      "lon_mean, lon_std = df['longitude'].mean(), df['longitude'].std()\n",
      "\n",
      "print max(df['latitude'] - lat_mean)\n",
      "invalid_lats = df.index[abs(df['latitude'] - lat_mean) > lat_std*2]\n",
      "print invalid_lats\n",
      "df['latitude'].loc[invalid_lats] = float('nan')\n",
      "invalid_lons = df.index[abs(df['longitude'] - lon_mean) > lon_std*2]\n",
      "df['longitude'].loc[invalid_lons] = float('nan')\n",
      "\n",
      "%matplotlib inline \n",
      "plt.plot(df['latitude'])\n",
      "plt.figure()\n",
      "plt.plot(df['longitude'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Indexing and selecting data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Basic indexing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here there be content"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Grouping data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here there be content"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Analysing and visualising data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Plotting timeseries data with Seaborn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here there be content"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Deriving new channels from data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here there be content"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "General tips"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Be careful with units\n",
      "- Code first, then optimize\n",
      "    - But, use loops sparingly\n",
      "- Don't reinvent the wheel\n",
      "- Make sure what you do to your data makes sense\n",
      "- Make sure you compare like-for-like\n",
      "- Plot and plot some more\n",
      "- Correctness is important, write unit tests!\n",
      "    - Writing good tests is *hard*. Debugginging code without test coverage is *harder*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}